# Awesome-MLLM-Reasoning-Benchmarks



#### Mathematical Reasoning
- WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?
  - Paper: https://arxiv.org/pdf/2407.01284
  - Dataset: https://huggingface.co/datasets/We-Math/We-Math
- (ICLR 2024 Oral) Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models
  - Paper: https://arxiv.org/pdf/2310.02255
  - Project Page: https://mathvista.github.io
  - Code: https://github.com/lupantech/MathVista
  - Dataset: https://huggingface.co/datasets/AI4Math/MathVista
  - LeaderBoard: https://mathvista.github.io/#leaderboard
- (ECCV 2024) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems
  - Paper: https://arxiv.org/pdf/2403.14624
  - Project Page: https://mathverse-cuhk.github.io
  - Code: https://github.com/ZrrSkywalker/MathVerse
  - Dataset: https://huggingface.co/datasets/AI4Math/MathVerse
  - LeaderBoard: https://mathverse-cuhk.github.io/#leaderboard
- (NeurIPS DB Track 2024) MATH-Vision: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
  - Paper: https://arxiv.org/pdf/2402.14804
  - Project Page: https://mathllm.github.io/mathvision
  - Code: https://github.com/mathllm/MATH-V
  - Dataset: https://huggingface.co/datasets/MathLLMs/MathVision
  - LeaderBoard: https://mathllm.github.io/mathvision/#leaderboard
- MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark
  - Paper: https://arxiv.org/pdf/2408.07543
  - Code: https://github.com/PKU-Baichuan-MLSystemLab/MathScape?tab=readme-ov-file
  - Dataset: https://drive.google.com/file/d/1Y3cnKPyryM0_m5QJQIOkF09KjDO9Q_QH/view
- CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of LMMs 【中文】
  - Paper: https://arxiv.org/pdf/2409.02834
  - https://github.com/ECNU-ICALK/EduChat-Math/
- OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
  - Paper: https://arxiv.org/pdf/2402.14008
  - Dataset: https://huggingface.co/datasets/Hothan/OlympiadBench


#### Chart reasoning
- ChartBench: A Benchmark for Complex Visual Reasoning in Charts
  - https://arxiv.org/pdf/2312.15915
  - https://chartbench.github.io/
  - https://huggingface.co/datasets/SincereX/ChartBench
- MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems
  - https://arxiv.org/pdf/2410.14179v2
  - https://github.com/Zivenzhu/Multi-chart-QA


#### Scientific Reasoning

- M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models
  - https://arxiv.org/pdf/2405.15638
  - https://m4u-benchmark.github.io/m4u.github.io/
  - https://huggingface.co/datasets/M4U-Benchmark/M4U
- MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
  - https://arxiv.org/pdf/2311.16502
  - https://huggingface.co/datasets/MMMU/MMMU
- MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark
  - https://arxiv.org/pdf/2409.02813
  - https://huggingface.co/datasets/MMMU/MMMU_Pro
- Science qa : Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering
  - https://arxiv.org/pdf/2209.09513
  - https://huggingface.co/datasets/derek-thomas/ScienceQA
- TheoremQA: A Theorem-driven Question Answering Dataset
  - https://arxiv.org/pdf/2305.12524
  - https://huggingface.co/datasets/TIGER-Lab/TheoremQA
- Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark
  - https://arxiv.org/pdf/2501.05444v1
  - https://huggingface.co/datasets/luckychao/EMMA
- GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
  - https://arxiv.org/pdf/2402.15745
  - https://github.com/OpenMOSS/GAOKAO-MM
- OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems [中英文]
  - https://arxiv.org/pdf/2402.14008
  - https://huggingface.co/datasets/Hothan/OlympiadBench
- CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark [中文]
  - https://arxiv.org/pdf/2401.11944
  - https://huggingface.co/datasets/m-a-p/CMMMU

#### Code Generation

- ChartMimic: Evaluating LMM’s Cross-Modal Reasoning Capability via Chart-to-Code Generation
  - https://arxiv.org/pdf/2406.09961
  - https://github.com/ChartMimic/ChartMimic
  - https://huggingface.co/datasets/ChartMimic/ChartMimic
- Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots 
  - https://arxiv.org/pdf/2405.07990
  - https://huggingface.co/datasets/TencentARC/Plot2Code
- HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks
  - https://arxiv.org/pdf/2410.12381
  - https://github.com/HumanEval-V/HumanEval-V-Benchmark
  - https://humaneval-v.github.io/#leaderboard
  - https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark


#### Multi-Image Based Inductive Reasoning
- MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models
  - https://arxiv.org/pdf/2502.00698
  - https://huggingface.co/datasets/huanqia/MM-IQ
  
- LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts
  - https://arxiv.org/pdf/2407.04973
  - https://github.com/Yijia-Xiao/LogicVista
- The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles
  - https://arxiv.org/pdf/2502.01081
  - https://github.com/declare-lab/LLM-PuzzleTest/


#### Social and Cultural Knowledge Reasoning
- Computational Meme Understanding: A Survey
  - https://aclanthology.org/2024.emnlp-main.1184.pdf
  - https://www.newyorker.com/news/annals-of-communications/the-meme-ification-of-american-politics
  
- II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models
  - https://arxiv.org/pdf/2406.05862
  - https://huggingface.co/datasets/m-a-p/II-Bench

- Can MLLMs Understand the Deep Implication Behind Chinese Images? [中文]
  - https://arxiv.org/pdf/2410.13854
  - https://cii-bench.github.io/
  - https://huggingface.co/datasets/m-a-p/CII-Bench
  - https://cii-bench.github.io/#leaderboard

- PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension
  - https://arxiv.org/pdf/2412.11906
  
- GPT-4V(ision) as A Social Media Analysis Engine
  - https://arxiv.org/pdf/2311.07547
  - https://github.com/VIStA-H/GPT-4V_Social_Media
  
- Geolocation with Real Human Gameplay Data:A Large-Scale Dataset and Human-Like Reasoning Framework
  - https://arxiv.org/pdf/2502.13759


#### Algorithmic Problem
- NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
  - https://arxiv.org/pdf/2403.01777
  - https://github.com/lizhouf/NPHardEval4V



#### Action Prediction

- Autonomous Driving
    - Exploring the Potential of Multi-Modal AI for Driving Hazard Prediction
    - https://arxiv.org/pdf/2310.04671v4
    - https://github.com/DHPR-dataset/DHPR-dataset
    - https://huggingface.co/datasets/DHPR/Driving-Hazard-Prediction-and-Reasoning
    
- Robot Manipulation
    - A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
        - https://arxiv.org/pdf/2502.08643
        - https://github.com/shivanshpatel35/IKER
        - https://iker-robot.github.io/
        - https://simpler-env.github.io/

- Gui Agent
    - InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection
        - https://arxiv.org/pdf/2501.04575
        - https://github.com/Reallm-Labs/InfiGUIAgent
        - https://huggingface.co/datasets/Reallm-Labs/InfiGUIAgent-Data
    - Mind2Web: Towards a Generalist Agent for the Web
        - https://arxiv.org/abs/2306.06070
        - https://osu-nlp-group.github.io/Mind2Web/
        - https://github.com/OSU-NLP-Group/Mind2Web
        - https://huggingface.co/datasets/osunlp/Mind2Web
    
    - SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents
        - https://arxiv.org/pdf/2401.10935



#### Spatial Reasoning
- Spatial Planing
    - Imagine while Reasoning in Space: Multimodal Visualization-of-Thought
        - https://arxiv.org/pdf/2501.07542
    - iVISPAR — An Interactive Visual-Spatial Reasoning Benchmark for VLMs 
        - https://arxiv.org/pdf/2502.03214v1
        - https://github.com/SharkyBamboozle/iVISPAR

- Spatial Relationship
    - PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial Reasoning of Large Multimodal Models
        - https://www.arxiv.org/pdf/2502.08636
    - Defining and Evaluating Visual Language Models’ Basic Spatial Abilities:A Perspective from Psychometrics
        - https://arxiv.org/pdf/2502.11859


#### Other Comprehensive MM Reasoning Benchmarks
- M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought
  - https://arxiv.org/pdf/2405.16473
  - https://huggingface.co/datasets/LightChen2333/M3CoT
- MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action
  - https://arxiv.org/pdf/2303.11381
  - https://github.com/microsoft/MM-REACT
- MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency
  - https://mmecot.github.io/
  - https://arxiv.org/pdf/2502.09621
  - https://huggingface.co/datasets/CaraJ/MME-CoT
- LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs
  - https://arxiv.org/pdf/2501.06186
  - https://huggingface.co/datasets/omkarthawakar/VRC-Bench
- ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models
  - https://arxiv.org/pdf/2502.09696
  - https://zerobench.github.io/
- R1-onevision
  - https://yangyi-vai.notion.site/r1-onevision
  - https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision-Bench
