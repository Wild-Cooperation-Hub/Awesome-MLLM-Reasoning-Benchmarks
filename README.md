# Awesome MLLM Reasoning Benchmarks

A Comprehensive Survey on Evaluating Reasoning Capabilities in Multimodal Large Language Models.

<p align="left">
  <b>Authors: <a href="https://scholar.google.com/citations?user=dqLvhvIAAAAJ&hl=zh-CN">Yaya Shi</a> and <a href="https://scholar.google.com/citations?user=qtdueToAAAAJ&hl=en">Zongyang Ma</a></b>
</p>



### Table of Contents
- [Awesome MLLM Reasoning Benchmarks](#awesome-mllm-reasoning-benchmarks)
    - [Mathematical Reasoning](#mathematical-reasoning)
    - [Chart reasoning](#chart-reasoning)
    - [Scientific Reasoning](#scientific-reasoning)
    - [Code Generation](#code-generation)
    - [Multi-Image Based Inductive Reasoning](#multi-image-based-inductive-reasoning)
    - [Social and Cultural Knowledge Reasoning](#social-and-cultural-knowledge-reasoning)
    - [Algorithmic Problem](#algorithmic-problem)
    - [Action Prediction](#action-prediction)
    - [Spatial Reasoning](#spatial-reasoning)
    - [Other Comprehensive MM Reasoning Benchmarks](#other-comprehensive-mm-reasoning-benchmarks)


### Mathematical Reasoning
- WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?
  - ğŸ“œ Paper: https://arxiv.org/pdf/2407.01284
  - ğŸ¤— Dataset: https://huggingface.co/datasets/We-Math/We-Math

- (ICLR 2024 Oral) Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2310.02255
  - ğŸ“š Project: https://mathvista.github.io
  - ğŸ’» Code: https://github.com/lupantech/MathVista
  - ğŸ¤— Dataset: https://huggingface.co/datasets/AI4Math/MathVista
  - ğŸ† LeaderBoard: https://mathvista.github.io/#leaderboard
  
- (ECCV 2024) MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems
  - ğŸ“œ Paper: https://arxiv.org/pdf/2403.14624
  - ğŸ“š Project: https://mathverse-cuhk.github.io
  - ğŸ’» Code: https://github.com/ZrrSkywalker/MathVerse
  - ğŸ¤— Dataset: https://huggingface.co/datasets/AI4Math/MathVerse
  - ğŸ† LeaderBoard: https://mathverse-cuhk.github.io/#leaderboard

- (NeurIPS DB Track 2024) MATH-Vision: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
  - ğŸ“œ Paper: https://arxiv.org/pdf/2402.14804
  - ğŸ“š Project: https://mathllm.github.io/mathvision
  - ğŸ’» Code: https://github.com/mathllm/MATH-V
  - ğŸ¤— Dataset: https://huggingface.co/datasets/MathLLMs/MathVision
  - ğŸ† LeaderBoard: https://mathllm.github.io/mathvision/#leaderboard

- MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark
  - ğŸ“œ Paper: https://arxiv.org/pdf/2408.07543
  - ğŸ’» Code: https://github.com/PKU-Baichuan-MLSystemLab/MathScape?tab=readme-ov-file
  - ğŸ¤— Dataset: https://drive.google.com/file/d/1Y3cnKPyryM0_m5QJQIOkF09KjDO9Q_QH/view

- CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of LMMs ã€ä¸­æ–‡ã€‘
  - ğŸ“œ Paper: https://arxiv.org/pdf/2409.02834
  - ğŸ’» Code: https://github.com/ECNU-ICALK/EduChat-Math/

- OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
  - ğŸ“œ Paper: https://arxiv.org/pdf/2402.14008
  - ğŸ¤— Dataset: https://huggingface.co/datasets/Hothan/OlympiadBench

- MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts
- ğŸ“œ Paper: https://arxiv.org/pdf/2502.20808
- ğŸ“š Project: https://eternal8080.github.io/MV-MATH.github.io/
- ğŸ’» Code: https://github.com/eternal8080/MV-MATH
- ğŸ¤— Dataset: https://huggingface.co/datasets/PeijieWang/MV-MATH

### Chart Reasoning
- ChartBench: A Benchmark for Complex Visual Reasoning in Charts
  - ğŸ“œ Paper: https://arxiv.org/pdf/2312.15915
  - ğŸ“š Project: https://chartbench.github.io/
  - ğŸ¤— Dataset: https://huggingface.co/datasets/SincereX/ChartBench
  
- MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems
  - ğŸ“œ Paper: https://arxiv.org/pdf/2410.14179v2
  - ğŸ’» Code: https://github.com/Zivenzhu/Multi-chart-QA

- (Neurips DB Track 2024) Chartxiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs
  - ğŸ“œ Paper: https://arxiv.org/pdf/2406.18521
  - ğŸ“š Project: https://charxiv.github.io/
  - ğŸ¤— Dataset: https://huggingface.co/datasets/princeton-nlp/CharXiv


### Scientific Reasoning

- M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2405.15638
  - ğŸ“š Project: https://m4u-benchmark.github.io/m4u.github.io/
  - ğŸ¤— Dataset: https://huggingface.co/datasets/M4U-Benchmark/M4U
  
- MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
  - ğŸ“œ Paper: https://arxiv.org/pdf/2311.16502
  - ğŸ¤— Dataset: https://huggingface.co/datasets/MMMU/MMMU
  
- MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark
  - ğŸ“œ Paper: https://arxiv.org/pdf/2409.02813
  - ğŸ¤— Dataset: https://huggingface.co/datasets/MMMU/MMMU_Pro
  
- Science qa : Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering
  - ğŸ“œ Paper: https://arxiv.org/pdf/2209.09513
  - ğŸ¤— Dataset: https://huggingface.co/datasets/derek-thomas/ScienceQA

- TheoremQA: A Theorem-driven Question Answering Dataset
  - ğŸ“œ Paper: https://arxiv.org/pdf/2305.12524
  - ğŸ¤— Dataset: https://huggingface.co/datasets/TIGER-Lab/TheoremQA

- Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark
  - ğŸ“œ Paper: https://arxiv.org/pdf/2501.05444v1
  - ğŸ¤— Dataset: https://huggingface.co/datasets/luckychao/EMMA

- GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
  - ğŸ“œ Paper: https://arxiv.org/pdf/2402.15745
  - ğŸ’» Code: https://github.com/OpenMOSS/GAOKAO-MM

- OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems [ä¸­è‹±æ–‡]
  - ğŸ“œ Paper: https://arxiv.org/pdf/2402.14008
  - ğŸ¤— Dataset: https://huggingface.co/datasets/Hothan/OlympiadBench

- CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark [ä¸­æ–‡]
  - ğŸ“œ Paper: https://arxiv.org/pdf/2401.11944
  - ğŸ¤— Dataset: https://huggingface.co/datasets/m-a-p/CMMMU

### Code Generation

- ChartMimic: Evaluating LMMâ€™s Cross-Modal Reasoning Capability via Chart-to-Code Generation
  - ğŸ“œ Paper: https://arxiv.org/pdf/2406.09961
  - ğŸ’» Code: https://github.com/ChartMimic/ChartMimic
  - ğŸ¤— Dataset: https://huggingface.co/datasets/ChartMimic/ChartMimic

- Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots 
  - ğŸ“œ Paper: https://arxiv.org/pdf/2405.07990
  - ğŸ¤— Dataset: https://huggingface.co/datasets/TencentARC/Plot2Code

- HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks
  - ğŸ“œ Paper: https://arxiv.org/pdf/2410.12381
  - ğŸ’» Code: https://github.com/HumanEval-V/HumanEval-V-Benchmark
  - ğŸ¤— Dataset: https://huggingface.co/datasets/HumanEval-V/HumanEval-V-Benchmark
  - ğŸ† LeaderBoard: https://humaneval-v.github.io/#leaderboard


### Multi-Image Based Inductive Reasoning
- MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2502.00698
  - ğŸ¤— Dataset: https://huggingface.co/datasets/huanqia/MM-IQ
  
- LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts
  - ğŸ“œ Paper: https://arxiv.org/pdf/2407.04973
  - ğŸ’» Code: https://github.com/Yijia-Xiao/LogicVista

- The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles
  - ğŸ“œ Paper: https://arxiv.org/pdf/2502.01081
  - ğŸ’» Code: https://github.com/declare-lab/LLM-PuzzleTest/


### Social and Cultural Knowledge Reasoning
- Computational Meme Understanding: A Survey
  - ğŸ“œ Paper: https://aclanthology.org/2024.emnlp-main.1184.pdf
  
- II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2406.05862
  - ğŸ¤— Dataset: https://huggingface.co/datasets/m-a-p/II-Bench

- Can MLLMs Understand the Deep Implication Behind Chinese Images? [ä¸­æ–‡]
  - ğŸ“œ Paper: https://arxiv.org/pdf/2410.13854
  - ğŸ“š Project: https://cii-bench.github.io/
  - ğŸ¤— Dataset: https://huggingface.co/datasets/m-a-p/CII-Bench
  - ğŸ† LeaderBoard: https://cii-bench.github.io/#leaderboard

- PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension
  - ğŸ“œ Paper: https://arxiv.org/pdf/2412.11906
  
- GPT-4V(ision) as A Social Media Analysis Engine
  - ğŸ“œ Paper: https://arxiv.org/pdf/2311.07547
  - ğŸ’» Code: https://github.com/VIStA-H/GPT-4V_Social_Media
  
- Geolocation with Real Human Gameplay Data:A Large-Scale Dataset and Human-Like Reasoning Framework
  - ğŸ“œ Paper: https://arxiv.org/pdf/2502.13759


### Algorithmic Problem
- NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2403.01777
  - ğŸ’» Code: https://github.com/lizhouf/NPHardEval4V



### Action Prediction

- Autonomous Driving
  - Exploring the Potential of Multi-Modal AI for Driving Hazard Prediction
      - ğŸ“œ Paper: https://arxiv.org/pdf/2310.04671v4
      - ğŸ’» Code: https://github.com/DHPR-dataset/DHPR-dataset
      - ğŸ¤— Dataset: https://huggingface.co/datasets/DHPR/Driving-Hazard-Prediction-and-Reasoning
      
- Robot Manipulation
  - A Real-to-Sim-to-Real Approach to Robotic Manipulation with VLM-Generated Iterative Keypoint Rewards
      - ğŸ“œ Paper: https://arxiv.org/pdf/2502.08643
      - ğŸ’» Code: https://github.com/shivanshpatel35/IKER
      - ğŸ“š Project: https://iker-robot.github.io/
      - ğŸ“š Project: https://simpler-env.github.io/

- Gui Agent
  - InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection
      - ğŸ“œ Paper: https://arxiv.org/pdf/2501.04575
      - ğŸ’» Code: https://github.com/Reallm-Labs/InfiGUIAgent
      - ğŸ¤— Dataset: https://huggingface.co/datasets/Reallm-Labs/InfiGUIAgent-Data

  - Mind2Web: Towards a Generalist Agent for the Web
      - ğŸ“œ Paper: https://arxiv.org/abs/2306.06070
      - ğŸ“š Project: https://osu-nlp-group.github.io/Mind2Web/
      - ğŸ’» Code: https://github.com/OSU-NLP-Group/Mind2Web
      - ğŸ¤— Dataset: https://huggingface.co/datasets/osunlp/Mind2Web

  - SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents
      - ğŸ“œ Paper: https://arxiv.org/pdf/2401.10935



### Spatial Reasoning
- Spatial Planing
    - Imagine while Reasoning in Space: Multimodal Visualization-of-Thought
        - ğŸ“œ Paper: https://arxiv.org/pdf/2501.07542
    
    - iVISPAR â€” An Interactive Visual-Spatial Reasoning Benchmark for VLMs 
        - ğŸ“œ Paper: https://arxiv.org/pdf/2502.03214v1
        - ğŸ’» Code: https://github.com/SharkyBamboozle/iVISPAR

- Spatial Relationship
    - PulseCheck457: A Diagnostic Benchmark for Comprehensive Spatial Reasoning of Large Multimodal Models
        - ğŸ“œ Paper: https://www.arxiv.org/pdf/2502.08636
    
    - Defining and Evaluating Visual Language Modelsâ€™ Basic Spatial Abilities:A Perspective from Psychometrics
        - ğŸ“œ Paper: https://arxiv.org/pdf/2502.11859


### Other Comprehensive MM Reasoning Benchmarks
- M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought
  - ğŸ“œ Paper: https://arxiv.org/pdf/2405.16473
  - ğŸ¤— Dataset: https://huggingface.co/datasets/LightChen2333/M3CoT

- MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action
  - ğŸ“œ Paper: https://arxiv.org/pdf/2303.11381
  - ğŸ’» Code: https://github.com/microsoft/MM-REACT

- MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for Reasoning Quality, Robustness, and Efficiency
  - ğŸ“œ Paper: https://arxiv.org/pdf/2502.09621
  - ğŸ“š Project: https://mmecot.github.io/
  - ğŸ¤— Dataset: https://huggingface.co/datasets/CaraJ/MME-CoT

- LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs
  - ğŸ“œ Paper: https://arxiv.org/pdf/2501.06186
  - ğŸ¤— Dataset: https://huggingface.co/datasets/omkarthawakar/VRC-Bench

- (Neurips DB Track 2024) MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs
  - ğŸ“œ Paper: https://arxiv.org/pdf/2407.16837
  - ğŸ“š Project: https://compbench.github.io/


- ZeroBench: An Impossible* Visual Benchmark for Contemporary Large Multimodal Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2502.09696
  - ğŸ“š Project: https://zerobench.github.io/

- R1-onevision
  - ğŸ“š Project: https://yangyi-vai.notion.site/r1-onevision
  - ğŸ¤— Dataset: https://huggingface.co/datasets/Fancy-MLLM/R1-Onevision-Bench

- LRMBench: A Comprehensive and Challenging Benchmark for Vision-Language Reward Models
  - ğŸ“œ Paper: https://arxiv.org/pdf/2411.17451
  - ğŸ“š Project: https://vl-rewardbench.github.io/
  - ğŸ’» Code: https://github.com/vl-rewardbench/VL_RewardBench
  - ğŸ¤— Dataset: https://huggingface.co/datasets/MMInstruction/VL-RewardBench
  - ğŸ† LeaderBoard: https://huggingface.co/spaces/MMInstruction/VL-RewardBench
 
### TODO
- â˜‘ï¸â˜‘ï¸â˜‘ï¸ Based on the aforementioned research, we are currently in the process of writing a survey paper and developing a benchmark. 
- ğŸ¤ğŸ¤ğŸ¤ We warmly invite everyone to join this collaborative project. Please feel free to reach out to us or submit pull requestsâ€”your contributions are highly valued!

  
